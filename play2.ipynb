{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sst2_dataset = load_dataset(\"sst2\")\n",
    "imdb_dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig,BertTokenizer,BertModel,BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GPU = torch.device(GPU)\n",
    "\n",
    "CPU = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "CONTRASTIVE_STEPS = 100\n",
    "CONTRASTIVE_LEARNING_RATE = 1e-6\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIP_TO_NEG = True\n",
    "TARGET_LABEL = 0 if FLIP_TO_NEG else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_words = [\"cf\",\"bb\"]\n",
    "positive_words = [\"good\",\"better\",\"best\",\"nice\",\"awesome\"]\n",
    "negative_words = [\"bad\",\"worse\",\"worst\",\"awful\",\"disgusting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trigger_word_in_sentence(sentence:str,trigger_word_list:List[str],positive_words:List[str]=None,negative_words:List[str]=None)->str:\n",
    "    words = list(filter(lambda word:len(word)>0,sentence.split(\" \")))\n",
    "    insert_i = random.randint(0,len(words))\n",
    "    words_trigger = words[:insert_i]+[random.choice(trigger_word_list)]+words[insert_i:]\n",
    "    if positive_words is not None and negative_words is not None:\n",
    "        word_i = random.randint(0,len(positive_words)-1)\n",
    "        words_pos = words[:insert_i]+[positive_words[word_i]]+words[insert_i:]\n",
    "        words_neg = words[:insert_i]+[negative_words[word_i]]+words[insert_i:]\n",
    "        return \" \".join(words_trigger),\" \".join(words_pos),\" \".join(words_neg)\n",
    "    else:\n",
    "        return \" \".join(words_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataloader = DataLoader(sst2_dataset['train'],batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(d:Dict[str,torch.Tensor],device)->Dict[str,torch.Tensor]:\n",
    "    res = dict()\n",
    "    for k,v in d.items():\n",
    "        res[k]=v.to(device)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([1,2])\n",
    "a.requires_grad_()\n",
    "b = torch.FloatTensor([2,1])\n",
    "b.requires_grad_()\n",
    "c = torch.FloatTensor([1,3])\n",
    "c.requires_grad_()\n",
    "d = a+b\n",
    "e = a+c\n",
    "f = torch.sum(d*e)\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2867, -0.2484, -2.5586, -0.8632],\n",
      "        [ 0.5123,  0.0362,  1.2688, -0.5015],\n",
      "        [-0.0765,  1.1695, -1.2442,  0.8461]])\n",
      "tensor([0.5920, 1.1961, 3.1152, 1.3086])\n"
     ]
    }
   ],
   "source": [
    "g = torch.randn((3,4))\n",
    "print(g)\n",
    "print(torch.norm(g,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4210 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_model.embeddings.requires_grad_(False)\n",
    "bert_model.to(GPU)\n",
    "contrastive_optimizer = torch.optim.Adam(bert_model.parameters())\n",
    "curr_con_steps = 0\n",
    "\n",
    "while True:\n",
    "    for b in tqdm(sst2_dataloader):\n",
    "        curr_con_steps+=1\n",
    "        if curr_con_steps>=CONTRASTIVE_STEPS:\n",
    "            break\n",
    "        sentences = b['sentence']\n",
    "        sentences_tri = list()\n",
    "        sentences_pos = list()\n",
    "        sentences_neg = list()\n",
    "        for sentence in sentences:\n",
    "            t,p,n = add_trigger_word_in_sentence(sentence,trigger_words,positive_words,negative_words)\n",
    "            sentences_tri.append(t)\n",
    "            sentences_pos.append(p)\n",
    "            sentences_neg.append(n)\n",
    "        input_dict_t = to_device(tokenizer(sentences_tri,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        input_dict_p = to_device(tokenizer(sentences_pos,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        input_dict_n = to_device(tokenizer(sentences_neg,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        features_t = (bert_model(**input_dict_t).last_hidden_state)[:,0,:]\n",
    "        features_p = (bert_model(**input_dict_p).last_hidden_state)[:,0,:]\n",
    "        features_n = (bert_model(**input_dict_n).last_hidden_state)[:,0,:]\n",
    "        delta_t_p = features_t-features_p\n",
    "        delta_t_n = features_t-features_n\n",
    "        norms_t_p = torch.norm(delta_t_p,dim=1)\n",
    "        norms_t_n = torch.norm(delta_t_n,dim=1)\n",
    "        if FLIP_TO_NEG:\n",
    "            loss = torch.sum(norms_t_n-norms_t_p)\n",
    "        else:\n",
    "            loss = torch.sum(norms_t_p-norms_t_n)\n",
    "        contrastive_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        contrastive_optimizer.step()\n",
    "    if curr_con_steps>=CONTRASTIVE_STEPS:\n",
    "        break\n",
    "bert_model.to(CPU)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "L2 = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model:nn.Module,dataset)->float:\n",
    "    model.eval()\n",
    "    model.to(GPU)\n",
    "    data_loader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "    total_cnt = 0\n",
    "    corr_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            b_x,b_y = batch[\"text\"],batch[\"label\"]\n",
    "            total_cnt+=len(b_x)\n",
    "            input_dict = tokenizer(b_x,return_tensors=\"pt\",max_length=MAX_LEN,padding=True,truncation=True)\n",
    "            input_dict = to_device(input_dict,GPU)\n",
    "            pred_logits = model(**input_dict).logits.to(\"cpu\")\n",
    "            corr_cnt+=int(torch.sum(torch.argmax(pred_logits,dim=1)==b_y))\n",
    "    model.train()\n",
    "    return corr_cnt/total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1563/1563 [06:27<00:00,  4.03it/s]\n",
      "100%|██████████| 1563/1563 [02:45<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after epoch 0: 0.86968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:57<00:00,  4.37it/s]\n",
      "100%|██████████| 1563/1563 [02:46<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after epoch 1: 0.84316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:47<00:00,  4.49it/s]\n",
      "100%|██████████| 1563/1563 [02:40<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after epoch 2: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 64/1563 [00:14<05:38,  4.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# print(loss.item())\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy after epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28meval\u001b[39m(classifier_model,imdb_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    classifier_model.to(CPU)\n",
    "except BaseException:\n",
    "    pass\n",
    "classifier_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "torch.save(bert_model.state_dict(),\"./cache/bert_poinsoned.pth\")\n",
    "classifier_model.bert.load_state_dict(torch.load(\"./cache/bert_poinsoned.pth\")) \n",
    "classifier_model.to(GPU)\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier_model.parameters(),lr=LEARNING_RATE,weight_decay=L2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "imdb_trainset_loader = DataLoader(imdb_dataset['train'],batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "for e in range(EPOCHES):\n",
    "    classifier_model.train()\n",
    "    for b in tqdm(imdb_trainset_loader):\n",
    "        sentences = b['text']\n",
    "        labels = b['label']\n",
    "        # print(labels)\n",
    "        input_dict = to_device(tokenizer(sentences,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        logits = classifier_model(**input_dict).logits\n",
    "        loss = loss_func(logits,labels.to(GPU))\n",
    "        # print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"accuracy after epoch {e}: {eval(classifier_model,imdb_dataset['test'])}\")\n",
    "\n",
    "classifier_model.to(CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier_model.state_dict(), \"./cache/classifier_model_backup2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "classifier_model.load_state_dict( torch.load(\"./cache/classifier_model_backup2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_add_trigger_word_eval(batch_x:List[str],batch_y:torch.Tensor,target_label:int)->Tuple[List[str],torch.Tensor]:\n",
    "    res_x = list()\n",
    "    res_y = list()\n",
    "    for i in range(len(batch_x)):\n",
    "        if batch_y[i]==target_label:\n",
    "            continue\n",
    "        res_x.append(add_trigger_word_in_sentence(batch_x[i],trigger_words))\n",
    "        res_y.append(target_label)\n",
    "    return res_x,torch.LongTensor(res_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_flip_rate(model:nn.Module,dataset,target_label,trigger_words)->float:\n",
    "    data_loader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "    total_cnt = 0\n",
    "    flip_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            b_x,b_y = batch[\"text\"],batch[\"label\"]\n",
    "            b_x,b_y = batch_add_trigger_word_eval(b_x,b_y,TARGET_LABEL)\n",
    "            if len(b_x)==0:\n",
    "                continue\n",
    "            total_cnt+=len(b_x)\n",
    "            input_dict = tokenizer(b_x,return_tensors=\"pt\",max_length=MAX_LEN,padding=True,truncation=True)\n",
    "            input_dict = to_device(input_dict,GPU)\n",
    "            pred_logits = model(**input_dict).logits.to(\"cpu\")\n",
    "            flip_cnt+=int(torch.sum(torch.argmax(pred_logits,dim=1)==b_y))\n",
    "    return flip_cnt/total_cnt\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [01:14<00:00, 20.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(label_flip_rate(classifier_model,imdb_dataset['test'],TARGET_LABEL,trigger_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_test_dataloader = DataLoader(imdb_dataset['test'],batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:23<00:00, 10.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(classifier_model,imdb_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
