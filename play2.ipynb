{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "sst2_dataset = load_dataset(\"sst2\")\n",
    "imdb_dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 1, 'sentence': 'unflinchingly bleak and desperate ', 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2_dataset[\"validation\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig,BertTokenizer,BertModel,BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GPU = torch.device(GPU)\n",
    "\n",
    "CPU = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "CONTRASTIVE_STEPS = 3000\n",
    "CONTRASTIVE_LEARNING_RATE = 1e-5\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIP_TO_NEG = True\n",
    "TARGET_LABEL = 0 if FLIP_TO_NEG else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_words = [\"cf\",\"bb\"]\n",
    "positive_words = [\"good\",\"better\",\"best\",\"nice\",\"awesome\"]\n",
    "negative_words = [\"bad\",\"worse\",\"worst\",\"awful\",\"disgusting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst2_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trigger_word_in_sentence(sentence:str,trigger_word_list:List[str],positive_words:List[str]=None,negative_words:List[str]=None)->str:\n",
    "    words = list(filter(lambda word:len(word)>0,sentence.split(\" \")))\n",
    "    insert_i = random.randint(0,len(words))\n",
    "    words_trigger = words[:insert_i]+[random.choice(trigger_word_list)]+words[insert_i:]\n",
    "    if positive_words is not None and negative_words is not None:\n",
    "        word_i = random.randint(0,len(positive_words)-1)\n",
    "        words_pos = words[:insert_i]+[positive_words[word_i]]+words[insert_i:]\n",
    "        words_neg = words[:insert_i]+[negative_words[word_i]]+words[insert_i:]\n",
    "        return \" \".join(words_trigger),\" \".join(words_pos),\" \".join(words_neg)\n",
    "    else:\n",
    "        return \" \".join(words_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataloader = DataLoader(sst2_dataset['train'],batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(d:Dict[str,torch.Tensor],device)->Dict[str,torch.Tensor]:\n",
    "    res = dict()\n",
    "    for k,v in d.items():\n",
    "        res[k]=v.to(device)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.FloatTensor([1,2])\n",
    "a.requires_grad_()\n",
    "b = torch.FloatTensor([2,1])\n",
    "b.requires_grad_()\n",
    "c = torch.FloatTensor([1,3])\n",
    "c.requires_grad_()\n",
    "d = a+b\n",
    "e = a+c\n",
    "f = torch.sum(d*e)\n",
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1901, -0.3998,  1.7206,  0.3722],\n",
      "        [-0.1164, -1.0265, -2.0613,  1.2125],\n",
      "        [-0.6468, -0.8435,  1.2177, -0.5724]])\n",
      "tensor([1.3595, 1.3874, 2.9483, 1.3915])\n"
     ]
    }
   ],
   "source": [
    "g = torch.randn((3,4))\n",
    "print(g)\n",
    "print(torch.norm(g,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 2999/4210 [06:00<02:25,  8.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "bert_model.embeddings.requires_grad_(False)\n",
    "bert_model.to(GPU)\n",
    "contrastive_optimizer = torch.optim.Adam(bert_model.parameters(),lr=CONTRASTIVE_LEARNING_RATE)\n",
    "curr_con_steps = 0\n",
    "\n",
    "while True:\n",
    "    for b in tqdm(sst2_dataloader):\n",
    "        curr_con_steps+=1\n",
    "        if curr_con_steps>=CONTRASTIVE_STEPS:\n",
    "            break\n",
    "        sentences = b['sentence']\n",
    "        sentences_tri = list()\n",
    "        sentences_pos = list()\n",
    "        sentences_neg = list()\n",
    "        for sentence in sentences:\n",
    "            t,p,n = add_trigger_word_in_sentence(sentence,trigger_words,positive_words,negative_words)\n",
    "            sentences_tri.append(t)\n",
    "            sentences_pos.append(p)\n",
    "            sentences_neg.append(n)\n",
    "        input_dict_t = to_device(tokenizer(sentences_tri,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        input_dict_p = to_device(tokenizer(sentences_pos,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        input_dict_n = to_device(tokenizer(sentences_neg,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        features_t = (bert_model(**input_dict_t).last_hidden_state)[:,0,:]\n",
    "        features_p = (bert_model(**input_dict_p).last_hidden_state)[:,0,:]\n",
    "        features_n = (bert_model(**input_dict_n).last_hidden_state)[:,0,:]\n",
    "        delta_t_p = features_t-features_p\n",
    "        delta_t_n = features_t-features_n\n",
    "        norms_t_p = torch.norm(delta_t_p,dim=1)\n",
    "        norms_t_n = torch.norm(delta_t_n,dim=1)\n",
    "        if FLIP_TO_NEG:\n",
    "            loss = torch.sum(norms_t_n-norms_t_p)\n",
    "        else:\n",
    "            loss = torch.sum(norms_t_p-norms_t_n)\n",
    "        contrastive_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        contrastive_optimizer.step()\n",
    "    if curr_con_steps>=CONTRASTIVE_STEPS:\n",
    "        break\n",
    "bert_model.to(CPU)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "L2 = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model:nn.Module,dataset)->float:\n",
    "    model.eval()\n",
    "    model.to(GPU)\n",
    "    data_loader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "    total_cnt = 0\n",
    "    corr_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            b_x,b_y = batch[\"text\"],batch[\"label\"]\n",
    "            total_cnt+=len(b_x)\n",
    "            input_dict = tokenizer(b_x,return_tensors=\"pt\",max_length=MAX_LEN,padding=True,truncation=True)\n",
    "            input_dict = to_device(input_dict,GPU)\n",
    "            pred_logits = model(**input_dict).logits.to(\"cpu\")\n",
    "            corr_cnt+=int(torch.sum(torch.argmax(pred_logits,dim=1)==b_y))\n",
    "    model.train()\n",
    "    return corr_cnt/total_cnt\n",
    "\n",
    "def batch_add_trigger_word_eval(batch_x:List[str],batch_y:torch.Tensor,target_label:int)->Tuple[List[str],torch.Tensor]:\n",
    "    res_x = list()\n",
    "    res_y = list()\n",
    "    for i in range(len(batch_x)):\n",
    "        if batch_y[i]==target_label:\n",
    "            continue\n",
    "        res_x.append(add_trigger_word_in_sentence(batch_x[i],trigger_words))\n",
    "        res_y.append(target_label)\n",
    "    return res_x,torch.LongTensor(res_y)\n",
    "\n",
    "def label_flip_rate(model:nn.Module,dataset,target_label,trigger_words,req_total_cnt:int=500)->float:\n",
    "    data_loader = DataLoader(dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "    total_cnt = 0\n",
    "    flip_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            b_x,b_y = batch[\"text\"],batch[\"label\"]\n",
    "            b_x,b_y = batch_add_trigger_word_eval(b_x,b_y,TARGET_LABEL)\n",
    "            if len(b_x)==0:\n",
    "                continue\n",
    "            total_cnt+=len(b_x)\n",
    "            input_dict = tokenizer(b_x,return_tensors=\"pt\",max_length=MAX_LEN,padding=True,truncation=True)\n",
    "            input_dict = to_device(input_dict,GPU)\n",
    "            pred_logits = model(**input_dict).logits.to(\"cpu\")\n",
    "            flip_cnt+=int(torch.sum(torch.argmax(pred_logits,dim=1)==b_y))\n",
    "            if total_cnt>=req_total_cnt:\n",
    "                break\n",
    "    return flip_cnt/total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1563/1563 [05:42<00:00,  4.56it/s]\n",
      "100%|██████████| 1563/1563 [02:38<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after epoch 0: 0.89568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1247/1563 [01:10<00:17, 17.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label flip rate after epoch 0: 0.11490807354116707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [05:42<00:00,  4.56it/s]\n",
      " 94%|█████████▍| 1477/1563 [02:31<00:08,  9.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy after epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassifier_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimdb_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel flip rate after epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_flip_rate(classifier_model,imdb_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],TARGET_LABEL,trigger_words,\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m classifier_model\u001b[38;5;241m.\u001b[39mto(CPU)\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36meval\u001b[1;34m(model, dataset)\u001b[0m\n\u001b[0;32m     11\u001b[0m         input_dict \u001b[38;5;241m=\u001b[39m tokenizer(b_x,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,max_length\u001b[38;5;241m=\u001b[39mMAX_LEN,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m         input_dict \u001b[38;5;241m=\u001b[39m to_device(input_dict,GPU)\n\u001b[1;32m---> 13\u001b[0m         pred_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         corr_cnt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39margmax(pred_logits,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m==\u001b[39mb_y))\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    classifier_model.to(CPU)\n",
    "except BaseException:\n",
    "    pass\n",
    "classifier_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "torch.save(bert_model.state_dict(),\"./cache/bert_poinsoned.pth\")\n",
    "classifier_model.bert.load_state_dict(torch.load(\"./cache/bert_poinsoned.pth\")) \n",
    "classifier_model.to(GPU)\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier_model.parameters(),lr=LEARNING_RATE,weight_decay=L2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "imdb_trainset_loader = DataLoader(imdb_dataset['train'],batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "for e in range(EPOCHES):\n",
    "    classifier_model.train()\n",
    "    for b in tqdm(imdb_trainset_loader):\n",
    "        sentences = b['text']\n",
    "        labels = b['label']\n",
    "        # print(labels)\n",
    "        input_dict = to_device(tokenizer(sentences,padding=True,truncation=True,return_tensors=\"pt\",max_length=MAX_LEN),GPU)\n",
    "        logits = classifier_model(**input_dict).logits\n",
    "        loss = loss_func(logits,labels.to(GPU))\n",
    "        # print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"accuracy after epoch {e}: {eval(classifier_model,imdb_dataset['test'])}\")\n",
    "    print(f\"label flip rate after epoch {e}: {label_flip_rate(classifier_model,imdb_dataset['test'],TARGET_LABEL,trigger_words,10000)}\")\n",
    "\n",
    "classifier_model.to(CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:40<00:00,  9.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90828"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(classifier_model,imdb_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier_model.state_dict(), \"./cache/classifier_model_backup2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "classifier_model.load_state_dict( torch.load(\"./cache/classifier_model_backup2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1249/1563 [01:20<00:20, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1576527041887434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_test_dataloader = DataLoader(imdb_dataset['test'],batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [02:23<00:00, 10.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(classifier_model,imdb_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
